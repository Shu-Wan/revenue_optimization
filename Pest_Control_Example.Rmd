---
title: "Pest Control"
author: "Lauren Kennedy, Jonah Gabry, & Rob Trangucci"
date: "July 23, 2018"
output:
  pdf_document: default
  html_document: default
---

## Setup

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(rstan)
library(dplyr)
library(reshape2)
library(bayesplot)
library(ggplot2)

theme_set(bayesplot::theme_default())

# for R's pseudo-RNGs, not Stan's
set.seed(1123) 
```

## The problem

Let's say that you're a data scientist working as an independent contractor.
You're contacted by the property manager of a large property network in New York
City. They explain that they are having an issue with the number of cockroach
complaints that they receive from their buildings. Previously they have offered
monthly visits from a pest inspector as a solution to this problem. While this
is the default solution of many property managers in NYC, the tenants are
rarely home when the inpector visits, and so the manager reasons that this is a
relatively expensive solution that is currently not very effective.

One alternative to this problem is to deploy long term bait stations. In this 
alternative child and pet safe bait stations are installed throughout the 
apartment building. Cockroaches obtain quick acting poison from these stations 
distribute it throughout the colony. The manufactorer for these bait stations 
provides some indication of the space to bait efficacy, but the manager suspects
that this guidance was not calculated for NYC roaches. NYC roaches, the manager 
rationalises, have more hustle than traditional roaches; and NYC buildings are 
built differently to other common domestic buildings in the US. This is 
particularly important as the unit cost for each bait station per year is quite
high.

The manager wishes to employ your services to help them to optimize the number 
of roach bait stations they should place in each of their buildings in order to 
minimise both the number of cockroach complaints and expenditure on pest 
control. They have randomly selected buildings. At the beginning
of each month, a pest inspector randomly places a number of bait stations 
throughout the building, without knowledge of the current cockroach levels in 
the building. At the end of the month, the manager records the total number of 
cockroach complaints in that building. Ideally, the manager would like to 
determine the optimal number of traps ($\textrm{traps}$) that balances the lost
revenue ($R$) that complaints ($\textrm{complaints}$) generate with the all-in
cost of maintaining the traps ($\textrm{TC}$). Luckily, we're up to the task! We
would formalize the problem like so:

$$
\arg\max_{\textrm{traps} \in \mathbb{N}} \mathbb{E}_c[R(\textrm{complaints}(\textrm{traps})) - \textrm{TC}(\textrm{traps})]
$$

The property manager would also, if possible, like to learn how these results 
generalize outside of the buildings they've treated so they can understand the
potential costs of pest control at buildings they are aquiring as well as for
the rest of their building portfolio.

As the property manager has complete control over the number of traps set, the
random variable contributing to this expectation is the number of complaints
given the number of traps. We will model the number of complaints as a function
of the number of traps.

## The data

The data is in a file called `building_data_20180724.RDS`. Let's load the data
and see what the structure is:

```{r load-data}
pest_data <- readRDS('data/building_data_20180727.RDS')
str(pest_data)
```

We have access to the following fields: 
* complaints: Number of complaints per building per month
* building_id: The unique building identifier
* traps: The number of traps used per month per building
* date: The date at which the number of complaints are recorded
* live_in_super: An indicator for whether the building as a live-in super
* age_of_building: The age of the building
* total_sq_foot: The total square footage of the building
* average_tenant_age: The average age of the tenants per building
* monthly_average_rent: The average monthly rent per building
* floors: The number of floors per building

First, let's see how many buildings we have data for:

```{r describe-data}
N_buildings <- length(unique(pest_data$building_id))
N_buildings
```

So far, so good, we've been given data for `r N_buildings` buildings.


```{r data-plots}
ggplot(pest_data, aes(x = complaints)) + 
  geom_bar()

ggplot(pest_data, aes(x = traps, y = complaints)) + 
  geom_col()

ggplot(pest_data, aes(x = traps, y = complaints, color = live_in_super == TRUE)) + 
  geom_jitter()

ggplot(pest_data, aes(x = date, y = complaints, color = live_in_super == TRUE)) + 
  geom_line(aes(linetype = "Number of complaints")) + 
  geom_point(color = "black") + 
  geom_line(aes(y = traps, linetype = "Number of traps"), color = "black", size = 0.25) + 
  facet_wrap(~building_id, scales = "free", ncol = 5) + 
  scale_x_date(name = "Month", date_labels = "%b") + 
  scale_y_continuous(name = "", limits = range(pest_data$complaints)) + 
  scale_linetype_discrete(name = "") + 
  scale_color_discrete(name = "Live-in super")
```




[Note: I'm going to move this block to later in the document when we
introduce hierarchical models, it's ok to leave here for now]

Let's prep the data. Firstly to use the building variable in Stan we will need
to trasnform it from a factor variable to an interger variable.

```{r prep-data}
N_months <- max(table(pest_data$building_id))
pest_data <- pest_data %>%
  mutate(building_fac = as.factor(building_id),
         building_idx = as.integer(building_fac),
         ids = rep(1:N_months, N_buildings),
         mo_idx = lubridate::month(date))
building_data <- 
  pest_data[,c('building_idx','live_in_super', 'age_of_building', 'total_sq_foot',
               'average_tenant_age', 'monthly_average_rent')] %>% unique() %>%
  arrange(building_idx) %>% select(-building_idx) %>% as.matrix() %>% 
  scale(scale=F) %>% sweep(MARGIN = 2, STATS=c(1,10,1e4,10,1e3),FUN = '/')
stan_dat_hier <- 
  with(pest_data, 
        list(complaints = complaints,
             traps = traps,
             N = length(traps),
             J = N_buildings,
             M = N_months,
             log_sq_foot = log(pest_data$total_sq_foot/1e4),
             building_data = building_data[,-3],
             mo_idx = as.integer(as.factor(date)),
             K = 4,
             building_idx = building_idx
             )
        )
```

The first question we might want to ask is whether the number of complaints per 
building per month is a function of the number of bait stations per building per
month. That requires only two variables, $\textrm{complaints}$ and 
$\textrm{traps}$. How can we model the number of complaints? 

## Modeling count data : Poisson distribution

We already know some rudimentary information about what we should expect. The
number of complaints over a month should be either zero or an integer. The
property manager tells us that it is possible but unlikely that number of
complaints in a given month is zero. Occasionally there are a very large number
of complaints in a single month. A common way of modelling this sort of skewed,
single bounded count data is as a Poisson random variable. One concern about
modelling the outcome variable as a Poisson is that the data may be
over-dispersed. How can we address this concern? We'll start with a simple
Poisson model and build it up slowly.

### Model 

Given that we have chosen a Poisson regression, we define the likelihood to be
the Poisson probability mass function over the number bait stations placed in
the building, denoted below as `traps`. This model assumes that the mean and
variance of the outcome variable `complaints` (number of complaints) is the
same. We'll investigate whether this is a good assumption after we fit the 
model.

For building $b = 1,\dots,10$ at time $t = 1,\dots,12$, we have

$$
\begin{align*}
\textrm{complaints}_{b,t} & \sim \textrm{Poisson}(\lambda_{b,t}) \\
\lambda_{b,t} & = \exp{(\eta_{b,t})} \\
\eta_{b,t} &= \alpha + \beta \, \textrm{traps}_{b,t}
\end{align*}
$$

## Bayesian workflow

Insert steps here


Let's encode this probability model in Stan code. 

### Writing our first Stan model

### Making sure our code is right

However, before we fit the model, we need to walk through
the best way to go about developing models in Stan.


### Simulate some data

How do we know if our Stan model is working well and if we are able to recover
the known parameter values? Before we start fitting our Poisson model to the
real data, first let's generate some fake data that matches our assumptions
about the data.

First we will compile the Stan model (simple_poisson_regression_dgp.stan) that generated the fake data.

```{r , cache=T, results="hide", message=FALSE}
comp_dgp_simple <- stan_model('stan_programs/simple_poisson_regression_dgp.stan')
```

Next we use this model to sample some data. 

```{r runpoissondgp}
fitted_model_dgp <- sampling(
  comp_dgp_simple,
  data = list(N = nrow(pest_data), mean_traps = mean(pest_data$traps)),
  chains = 1,
  iter = 1,
  algorithm = 'Fixed_param',
  seed = 123
  )
samps_dgp <- rstan::extract(fitted_model_dgp)
str(samps_dgp)
```

In order to pass the fake data to our Stan program using RStan, we need to
arrange the data into a named list, whose elements correspond to the names
in our Stan program.

```{r}
stan_dat_fake <- list(
  N = nrow(pest_data), 
  traps = samps_dgp$traps[1, ], 
  complaints = samps_dgp$complaints[1, ]
)
str(stan_dat_fake)
```
### Fit the model to the fake data:

Now we have the simulated data we fit a Stan model using it. First we need to compile the model (simple_poisson_regression.stan).

```{r , cache=T, results="hide", message=FALSE}
comp_model_P <- stan_model('stan_programs/simple_poisson_regression.stan')
```

Lastly, let's run the model to see if we can recover our simulated parameters. 

```{r}
fit_model_P <- sampling(comp_model_P, data = stan_dat_fake)

# see http://mc-stan.org/rstan/articles/stanfit_objects.html for various
# ways of extracting the contents of the stanfit object
posterior_alpha_beta <- as.matrix(fit_model_P, pars = c('alpha','beta'))
head(posterior_alpha_beta)
```

### Assess parameter recovery

First explore if we can recover the data that we originally simulated from.

```{r}
true_alpha_beta <- c(samps_dgp$alpha, samps_dgp$beta)
mcmc_recover_hist(posterior_alpha_beta, true = true_alpha_beta)
```

Then do Posterior Predictive Checks

```{r}
y_rep <- as.matrix(fit_model_P, pars = "y_rep")
mean_y_rep <- colMeans(y_rep)
std_resid <- (stan_dat_fake$complaints - mean_y_rep) / sqrt(mean_y_rep)
qplot(mean_y_rep, std_resid) + hline_at(1) + hline_at(-1)
```
This is a plot of... If the model was fitting well it would look like... As you can see here, we have ... which indicates that ... Finally look at the rootogram. 

```{r}
ppc_rootogram(stan_dat_fake$complaints, yrep = y_rep)
```
This is a plot of... If the model was fitting well it would look like... As you can see here, we have ... which indicates that ...

### Fit with real data 

Now we have seen that we can sensibly recover the parameters from simulated
data. Next we can use this model with the real data that we observerd. We'll again
pass the data to RStan as a list:

```{r stan-data}
stan_dat_simple <- list(
  N = nrow(pest_data), 
  complaints = pest_data$complaints,
  traps = pest_data$traps
)
```

As we have already compiled the model, we can jump straight to sampling from it.

```{r fit_P_real_data, cache=T}
fit_P_real_data <- sampling(comp_model_P, data = stan_dat_simple, chains = 4, cores =4 )
```
and printing the parameters. What do these tell us? 

```{r results_simple_P}
print(fit_P_real_data, pars = c('alpha','beta'))
```

```{r results_simple_P}
mcmc_hist(as.matrix(fit_P_real_data, pars = c('alpha','beta')))
```

As we expected, it appears the number of bait stations set in a building impacts the number of complaints about cockroaches that were made in the follwoing month. However, we still need to consider how well the model fits. 

```{r marginal_PPC}
y_rep <- as.matrix(fit_P_real_data, pars = "y_rep")
ppc_dens_overlay(y = stan_dat_simple$complaints, y_rep[1:200,])
```
This is a plot of the kernel densities of the observed data ($y$) and 200 simulations of $y$ from the posterior predictive distribution ($y_rep$). If the model was fitting the data well, there would be little difference between the observed density and the simulated density. However, as you can see here, the simulated density is not as disperse as the observed, and doesn't seem to capture the rate of zeros in the observed data. This indicates that the Poisson model may not be the best model to use for this data. Let's explore this further. 


```{r}
mean_y_rep <- colMeans(y_rep)
std_resid <- (stan_dat_simple$complaints - mean_y_rep) / sqrt(mean_y_rep)
qplot(mean_y_rep, std_resid) + hline_at(1) + hline_at(-1)
```
This is a plot of the standardised residuals of the observed vs predicted number of complaints. Ideally these would be centered around zero on the y-axis with the bulk of the residuals between -1 and 1 (1 standard deviation on the unit scale).  As you can see here, it looks as though we have more positive residuals than negative, which indicates that the model tends to underestimate the number of complaints that will be received. 

```{r}
ppc_rootogram(stan_dat_simple$complaints, yrep = y_rep)
```
The rootogram is another useful plot to compare the observed vs expected number of complaints. This is a plot of the expected counts (continuous line) vs the observed counts (blue histogram). If the model was fitting well these would be relatively similar, however in this figure we can see the number of complaints is underestimated if there are few complaints, over-estimated for medium numbers of complaints and underestimated if there are a large number of complaints. 

We can also view how the predicted number of complaints varies with the number of traps. From this we can see that the predicted number of complaints doesn't seem to fully capture the observed number of complaints. 

```{r}
ppc_intervals(
  y = stan_dat_simple$complaints, 
  yrep = y_rep,
  x = stan_dat_simple$traps
) + 
  labs(x = "Number of traps", y = "Number of complaints")
```

### Is there more info we can use?

Modeling the relationship between complaints and bait stations is the simplest model. However, the manager has told us that they expect there are a number of other reasons that one building might have more roach complaints than another. 

#### Data

Using the property manager's intuition, we include two extra peices of information we know about the building - the (log of the) square floor space and whether there is a live in super or not - into both the simulated and real data. A quick test shows us that there appears to be a relationship between the square footage of the building and the number of complaints received:

```{r}
sq_foot_dat_lm <- lm(log1p(complaints) ~ log(total_sq_foot), data = pest_data)
with(pest_data, plot(log(total_sq_foot), log1p(complaints)))
abline(reg = sq_foot_dat_lm)
```

 
```{r}
stan_dat_simple$log_sq_foot <- log(pest_data$total_sq_foot/1e4)
stan_dat_simple$live_in_super <- pest_data$live_in_super
```


####Model

Now we need a new Stan model that uses multiple predictors. Firs we have to compile multiple_poisson_regression_dgp.stan.

```{r compmultPDGP, cache=T, results="hide", message=FALSE}
comp_dgp_multiple <- stan_model('stan_programs/multiple_poisson_regression_dgp.stan')
```

Then we generate some fake data from this model. 

```{r runpoissondgp2}
fitted_model_dgp <-
  sampling(
  comp_dgp_multiple,
  data = list(N = nrow(pest_data)),
  chains = 1,
  cores = 1,
  iter = 1,
  algorithm = 'Fixed_param',
  seed = 123
  )
samps_dgp <- rstan::extract(fitted_model_dgp)
```

Now pop that data into a list ready for Stan. 

```{r}
stan_dat_fake <- list(
  N = nrow(pest_data), 
  log_sq_foot = samps_dgp$log_sq_foot[1, ],
  live_in_super = samps_dgp$live_in_super[1, ],
  traps = samps_dgp$traps[1, ], 
  complaints = samps_dgp$complaints[1, ]
)
```

And compile the model to fit this data with. 

```{r, cache=T, results="hide", message=FALSE}
comp_model_P_mult <- stan_model('stan_programs/multiple_poisson_regression.stan')
```

Fit the data with the model, and extract the alpha and beta parameters. 

```{r}
fit_model_P_mult <- sampling(comp_model_P_mult, data = stan_dat_fake, chains = 4, cores = 4)
posterior_alpha_beta <- as.matrix(fit_model_P_mult, pars = c('alpha','beta','beta_super','beta_sq_foot'))
```
Then compare these parameters to the true parameters

```{r}
true_alpha_beta <- c(samps_dgp$alpha,samps_dgp$beta,samps_dgp$beta_super,samps_dgp$beta_sq_foot)
mcmc_recover_hist(posterior_alpha_beta, true = true_alpha_beta)
```
This is a plot of... If the model was fitting well it would look like... As you can see here, we have ... which indicates that ...

### Fit the real data

Now let's fit to real data.

First fit the real data with the model.

```{r fit_mult_P_real_dat}
fit_model_P_mult_real <- sampling(comp_model_P_mult, data = stan_dat_simple, cores = 4, chains = 4)
```
Then explore the fit. 

```{r}
y_rep <- as.matrix(fit_model_P_mult_real, pars = "y_rep")
ppc_dens_overlay(stan_dat_simple$complaints, y_rep[1:200,])
```
Again we plot the observed kernel density against the estimated kernel density and see that there are the same discrpencies as before.

```{r}
mean_y_rep <- colMeans(y_rep)
std_resid <- (stan_dat_simple$complaints - mean_y_rep) / sqrt(mean_y_rep)
qplot(mean_y_rep, std_resid) + hline_at(1) + hline_at(-1)
```
We also see the same pattern of standardized residuals as with the single predictor model. 


```{r}
ppc_rootogram(stan_dat_simple$complaints, yrep = y_rep)
```
Lastly when we consider the rootogram, we again see the same pattern as the model with a single predictor. 


```{r}
ppc_intervals(
  y = stan_dat_simple$complaints, 
  yrep = y_rep,
  x = stan_dat_simple$traps
) + 
  labs(x = "Number of traps", y = "Number of complaints")
```

## Modeling count data : Negative Binomial outcome

When we considered modelling the data using a Poisson, we saw that the model didn't appear to fit as well to the data as we would like. In particular the model underpredicted low and high numbers of complaints, and overpredicted the medium number of complaints. This is one indication of over-dispersion, where the variance is larger than the mean. A Poisson model doesn't fit over-dispersed count data very well because the same parameter $\lambda$, controls both the expected counts and the variance of these counts. The natural alternative to this is the negative binomial model: 

$$
\begin{align*}
\text{complaints}_{b,t} & \sim \text{Neg-Binomial}(\lambda_{b,t}, \phi) \\
\lambda_{b,t} & = \exp{(\eta_{b,t})} \\
\eta_{b,t} &= \alpha + \beta_1 {\rm traps} + \beta_2 {\rm super} + \beta_3 \text{log_sq_foot}
\end{align*}
$$

In Stan the negative binomial mass function we'll use is called 
$\texttt{neg_binomial_2_log}(\text{ints} \, y, \text{reals} \, \eta, \text{reals} \, \phi)$ in Stan. Like the `poisson_log` function, this negative binomial mass function that is parameterized in terms of its 
log-mean, $\eta$, but it also has a precision $\phi$ such that

$$
\mathbb{E}[y] \, = \lambda = \exp(\eta)
$$

$$
\text{Var}[y] = \lambda + \lambda^2/\phi = \exp(\eta) + \exp(\eta)^2 / \phi.
$$ 
As $\phi$ gets larger the term $\lambda^2 / \phi$ gets smaller and the negative-binomial gets closer and closer to the Poisson.


###Fake data fit: Multiple NB regression

```{r , cache=T, results="hide", message=FALSE}
comp_dgp_multiple_NB <- stan_model('stan_programs/multiple_NB_regression_dgp.stan')
```

We're going to generate one draw from the fake data model so we can use the data to fit our model and compare the known values of the parameters to the posterior density of the parameters.

```{r fake_data_dgp_NB}
fitted_model_dgp_NB <-
  sampling(
  comp_dgp_multiple_NB,
  data = list(N = nrow(pest_data)),
  chains = 1,
  cores = 1,
  iter = 1,
  algorithm = 'Fixed_param',
  seed = 123
  )
samps_dgp_NB <- rstan::extract(fitted_model_dgp_NB)
```

Create a dataset to feed into the Stan model.

```{r NB_fake_stan_dat}
stan_dat_fake_NB <- list(
  N = nrow(pest_data), 
  log_sq_foot = samps_dgp_NB$log_sq_foot[1, ],
  live_in_super = samps_dgp_NB$live_in_super[1, ],
  traps = samps_dgp_NB$traps[1, ], 
  complaints = samps_dgp_NB$complaints[1, ]
)
```

Compile the inferential model.

```{r , cache=T, results="hide", message=FALSE}
comp_model_NB <- stan_model('stan_programs/multiple_NB_regression.stan')
```

Now we run our NB regression over the fake data and extract the samples to examine posterior predictive checks and to check whether we've sufficiently recovered our known parameters, $\texttt{alpha}$ $\texttt{beta}$.

```{r runNBoverfake}
fitted_model_NB <- 
  sampling(comp_model_NB,
           data = stan_dat_fake_NB,
           chains = 4, cores = 4
  )
posterior_alpha_beta_NB <- 
  as.matrix(fitted_model_NB,
            pars = c('alpha','beta',
                     'beta_super',
                     'beta_sq_foot',
                     'inv_prec')
  )
```

Construct the vector of true values from your simulated dataset and compare to the recovered parameters. 
```{r}
true_alpha_beta_NB <- 
  c(samps_dgp_NB$alpha,
    samps_dgp_NB$beta,
    samps_dgp_NB$beta_super,
    samps_dgp_NB$beta_sq_foot,
    samps_dgp_NB$inv_prec
  )
mcmc_recover_hist(posterior_alpha_beta_NB, true = true_alpha_beta_NB)
```

[Note to Lauren and Jonah: This looks OK, but what's going on with the 
intercept? I think we need to shift the intercept to deal with the effect from
noncentered parameters, but I'm not sure. It looks like the posterior standard
deviation is larger than the prior standard deviation. Not impossible, but 
still odd for the overall intercept. Also, the 
inverse precision parameter has almost no contraction, which is odd. We should
figure this out before the class]

```{r}
y_rep <- rstan::extract(fitted_model_NB)$y_rep
mean_inv_prec <- mean(posterior_alpha_beta_NB[,'inv_prec'])
mean_y_rep <- colMeans(y_rep)
std_resid <- (stan_dat_simple$complaints - mean_y_rep) / sqrt(mean_y_rep + mean_y_rep^2*mean_inv_prec)
qplot(mean_y_rep, std_resid) + hline_at(1) + hline_at(-1)
```

Fit the model to the real data.

```{r runNB}
fitted_model_NB <- sampling(comp_model_NB, data = stan_dat_simple, chains = 4, cores = 4)
samps_NB <- rstan::extract(fitted_model_NB)
```

Let's look at our predictions vs. the data.

```{r ppc-full}
y_rep <- samps_NB$y_rep
ppc_dens_overlay(stan_dat_simple$complaints, 
                            y_rep[1:200,])
```

These look OK, but let's look at the standardized residual plot.

```{r}
mean_inv_prec <- mean(samps_NB$inv_prec)
mean_y_rep <- colMeans(y_rep)
std_resid <- (stan_dat_simple$complaints - mean_y_rep) / sqrt(mean_y_rep + mean_y_rep^2*mean_inv_prec)
qplot(mean_y_rep, std_resid) + hline_at(1) + hline_at(-1)
```

Looks ok, but we still have some large residuals. This might be because we are currently ignoring that the data are clustered by buildings, and different buildings might have different overall probability of roaches. 

```{r}
ppc_rootogram(stan_dat_simple$complaints, yrep = y_rep)
```

The rootgram now looks much more plausible. We can tell this because now the expected number of complaints matches much closer to the observed number of complaints. 


Check predictions by number of traps: 
```{r}
ppc_intervals(
  y = stan_dat_simple$complaints, 
  yrep = y_rep,
  x = stan_dat_simple$traps
) + 
  labs(x = "Number of traps", y = "Number of complaints")
```


Check predictions building by building:
```{r ppc-group_means}
ppc_stat_grouped(
  stan_dat_simple$complaints, 
  y_rep, 
  group = pest_data$building_idx, 
  stat = 'mean'
)
```


### Hierarchical modeling
#### Random intercepts for each building

Previously we discussed that It appears that we aren't modeling the means well for each building. Let's add a hierarchical
intercept parameter, $\alpha_b$ at the building level to our model. 

\begin{align*}
\text{complaints}_{b,t} &\sim \text{Neg-Binomial-log}(\eta_{b,t}, \phi) \\
\eta_{b,t} &= \alpha_b + X_{b,t} \beta \\
\alpha_b &\sim \text{Normal}(\alpha, \sigma_{\alpha})
\end{align*}

In our stan model, $\alpha_b$ is the $b$-th element of the vector
$\texttt{alpha}$ defined in the parameter vector. 

We could also shove the building-only predictors into the model for 
$\alpha_b$, like so:

$$
\alpha_b \sim \text{Normal}(\alpha + \zeta \, \texttt{building_data}_b, \sigma_{\alpha})
$$

Let's compile the model. 

```{r comp-NB-hier, cache=T, results="hide", message=FALSE}
comp_model_NB_hier <- stan_model('stan_programs/hier_NB_regression.stan')
```

Fit the model to data.

```{r run-NB-hier}
fitted_model_NB_hier <- sampling(comp_model_NB_hier, data = stan_dat_hier, chains = 4, cores = 4)
```

We get a bunch of warnings from Stan about divergent transitions. Divergent transitions are...

Divergent transitions are something we need to invesigate further. In this example we will see that we have divergent transitions because we need to reparameterize our model - i.e., we will retain the overall structure of the model, but transform some of the parameters so that it is easier for Stan to sample from the parameter space. Before we go through exactly how to do this reparameterization, we will first go through what indicates that this is something that reparamerization will be useful for. We will go through:

1. Examining the fitted parameter values, including the effective sample size
2. Traces plots and pairs plots, which help us to see whether the divergent transitions have a pattern for any of the parameters. 

First let's extract the fits from the model. 

```{r} 
samps_hier_NB <- rstan::extract(fitted_model_NB_hier)
```

Then we print the fits for the parameters that are of most interest. 

```{r print-NB-hier}
print(fitted_model_NB_hier, pars = c('sigma_alpha','beta','alpha','prec','alphas'))
```
You can see that the effective samples are quite low for many of the parameters relative to the total number of samples. This alone isn't indicative of the need to reparameterize, but it indicates that we should look further at the trace plots and pairs plots. First let's look at the traceplots - 

```{r}
# use as.array to keep the markov chains separate for trace plots
mcmc_trace(
  as.array(fitted_model_NB_hier,pars = 'sigma_alpha'), 
  np = nuts_params(fitted_model_NB_hier)
)
```

```{r}
# assign to object so we can compare to another plot later
scatter_with_divs <- mcmc_scatter(
  as.array(fitted_model_NB_hier),
  pars = c("alphas[4]", 'sigma_alpha'), 
  transform = list('sigma_alpha' = "log"), 
  np = nuts_params(fitted_model_NB_hier)
)
scatter_with_divs
```

What we have here is a funnel, which Stan can't handle because...

```{r}
parcoord_with_divs <- mcmc_parcoord(
  as.array(fitted_model_NB_hier, pars = c("sigma_alpha", "alphas")),
  np = nuts_params(fitted_model_NB_hier)
)
parcoord_with_divs
```

Again, we see evidence that our problems concentrate when $\texttt{sigma_alpha}$ is small

Instead, we should use the non-centered parameterization for $\alpha_b$. We
define a vector of auxiliary variables in the parameters block,
$\texttt{alphas_raw}$ that is given a $\text{Normal}(0, 1)$ prior in the model
block. We then make $\texttt{alphas}$ a transformed parameter:
We can reparameterize the random intercept $\alpha_b$, which is distributed:

$$
\alpha_b \sim \text{Normal}(\alpha + \zeta \, \texttt{building_data}, \sigma_{\alpha})
$$



```
transformed parameters {
  vector[J] alphas;
  alphas = alpha + building_data * zeta + sigma_alpha * alphas_raw;
}
```

This gives $\texttt{alphas}$ a $\text{Normal}(\alpha + \texttt{building_data}\, \zeta, \sigma_\alpha)$ distribution, but it
decouples the dependence of the density of each element of $\texttt{alphas}$ from 
$\texttt{sigma_alpha}$ ($\sigma_\alpha$). hier_NB_regression_ncp.stan uses the non-centered
parameterization for $\texttt{alphas}$. We will examine the effective sample size of the
fitted model to see whether we've fixed the problem with our reparameterization.

Compile the model.

```{r comp-NB-hier-ncp, cache=TRUE}
comp_model_NB_hier_ncp <- stan_model('stan_programs/hier_NB_regression_ncp.stan')
```

Fit the model to the data.

```{r run-NB-hier-ncp}
fitted_model_NB_hier_ncp <- sampling(comp_model_NB_hier_ncp, data = stan_dat_hier, chains = 4, cores = 4)
```

Examining the fit of the new model

```{r n-eff-NB-hier-ncp-check}
print(fitted_model_NB_hier_ncp, pars = c('sigma_alpha','beta','alpha','prec','alphas'))
```

This has improved the effective sample sizes of $\texttt{alphas}$. We extract
the parameters to run our usual posterior predictive checks.

```{r}
scatter_no_divs <- mcmc_scatter(
  as.array(fitted_model_NB_hier_ncp),
  pars = c("alphas[4]", 'sigma_alpha'), 
  transform = list('sigma_alpha' = "log"), 
  np = nuts_params(fitted_model_NB_hier_ncp)
)
bayesplot_grid(scatter_with_divs, scatter_no_divs, 
               grid_args = list(ncol = 2), ylim = c(-11, -0.5))
```

```{r}
parcoord_no_divs <- mcmc_parcoord(
  as.array(fitted_model_NB_hier_ncp, pars = c("sigma_alpha", "alphas")),
  np = nuts_params(fitted_model_NB_hier_ncp)
)
bayesplot_grid(parcoord_with_divs, parcoord_no_divs, 
               ylim = c(-3, 3))
```

```{r samps-full-hier}
samps_NB_hier_ncp <- rstan::extract(fitted_model_NB_hier_ncp, pars = c('y_rep','inv_prec'))
```

The marginal plot, again.

```{r ppc-full-hier}
y_rep <- as.matrix(fitted_model_NB_hier_ncp, pars = "y_rep")
ppc_dens_overlay(stan_dat_hier$complaints, y_rep[1:200,])
```

This looks quite nice. If we've captured the building-level means well, then the
posterior distribution of means by building should match well with the observed
means of the quantity of building complaints by month.

```{r ppc-group_means-hier}
ppc_stat_grouped(
  y = stan_dat_hier$complaints, 
  yrep = y_rep,
  group = stan_dat_hier$building_idx, 
  stat = 'mean'
)
```

The building means appear to be well-captured by our model.


Predictions by number of traps:

```{r}
ppc_intervals_grouped(
  y = stan_dat_hier$complaints, 
  yrep = y_rep,
  x = stan_dat_hier$traps
) + 
  labs(x = "Number of traps", y = "Number of complaints")
```


Standardized residuals:
```{r}
mean_y_rep <- colMeans(y_rep)
mean_inv_prec <- mean(as.matrix(fitted_model_NB_hier_ncp, pars = "inv_prec"))
std_resid <- (stan_dat_hier$complaints - mean_y_rep) / sqrt(mean_y_rep + mean_y_rep^2*mean_inv_prec)
qplot(mean_y_rep, std_resid) + hline_at(1) + hline_at(-1)
```

Rootogram:
```{r}
ppc_rootogram(stan_dat_hier$complaints, yrep = y_rep)
```


Ok, so these plots look \emph{much} better. Perhaps if the levels of complaints
differ by building, the coefficient for the effect of traps on building does
too. We can add this to our model and observe the fit.


\begin{align*}
\text{complaints}_{b,t} &\sim \text{Neg-Binomial-log}(\eta_{b,t}, \phi)  \\
\eta_{b,t} &= \alpha_b + \beta_b \, \texttt{traps}_{b,t} \\
\alpha_b &\sim \text{Normal}(\alpha + \texttt{building_data} \, \zeta, \sigma_{\alpha}) \\
\beta_b &\sim \text{Normal}(\beta + \texttt{building_data} \, \gamma, \sigma_{\beta})
\end{align*}


Let's compile the model.

```{r comp-NB-hier-slopes, cache=T, results="hide", message=FALSE}
comp_model_NB_hier_slopes <- stan_model('stan_programs/hier_NB_regression_ncp_slopes_mod.stan')
```

Fit the model to data and extract the posterior draws needed for 
our posterior predictive checks.

```{r run-NB-hier-slopes}
fitted_model_NB_hier_slopes <- 
  sampling(
    comp_model_NB_hier_slopes,
    data = stan_dat_hier,
    chains = 4, cores = 4
  )
```

To see if the model infers building-to-building differences in, we can plot a histogram of our marginal
posterior distribution for $\texttt{sigma_beta}$. 

```{r}
mcmc_hist(
  as.matrix(fitted_model_NB_hier_slopes, pars = "sigma_beta"), 
  binwidth = 0.005
)
```

While the model can't specifically rule out zero from the posterior, it does have mass at small
non-zero numbers, so we should leave in the hierarchy over $\texttt{betas}$. Plotting the marginal
data density again, we can see the model still looks well calibrated.

```{r ppc-full-hier-slopes}
y_rep <- as.matrix(fitted_model_NB_hier_slopes, pars = "y_rep")
ppc_dens_overlay(
  y = stan_dat_hier$complaints, 
  yrep = y_rep[1:200,]
)
```

[It might be interesting to do more investingation of the model, but I haven't added this yet]
[We haven't done anything about inference really, we're just hammering in the posterior predictive
checks. This could motivate informative priors for some of the zeta/gamma regression parameters]

### Time varying effects and structured priors

We haven't looked at how cockroach complaints change over time. Let's look
at whether there's any pattern over time.

```{r ppc-group_max-hier-slopes-mean-by-mo}
ppc_stat_grouped(
  y = stan_dat_hier$complaints, 
  yrep = y_rep, 
  group = stan_dat_hier$mo_idx, 
  stat = 'mean'
) + xlim(0, 9)
```

There's definitely some pattern that we're not capturing. See October and May and June for examples.
If we think about the sort of prior we want to use for this parameter, we might settle on
an autoregressive prior.

[This doesn't quite come out from any residuals because we have so few weeks of data]

We might be missing information in month indicators. It makes sense, when it gets colder
out there might be less trash in the streets, which could mean fewer cockroaches are out
and about rooting through the trash and ending up in our buildings.

This can be a motivation for using an autoregressive prior for our monthly effects. The model
is as follows:

$$
\texttt{mo}_t \sim \text{Normal}(\phi \, \texttt{mo}_{t-1}, \sigma_\texttt{mo}), \quad \phi \in [0,1]
$$

While we could allow $\phi \in [-1,1]$, we almost never see $\phi < 0$ in social science
applications. We could relax this assumption and use priors to enforce our beliefs

[Note to L/J: we should probably just use priors to encode our
beliefs about positive autocorrelation this upon second thought]

```{r comp-NB-hier-mos, cache=T, results="hide", message=FALSE}
comp_model_NB_hier_mos <- stan_model('stan_programs/hier_NB_regression_ncp_slopes_mod_mos.stan')
```

```{r run-NB-hier-slopes-mos}
fitted_model_NB_hier_mos <- sampling(comp_model_NB_hier_mos, data = stan_dat_hier, chains = 4, cores = 4)
```


In the interest of brevity, we won't go on expanding the model. 

```{r ppc-full-hier-mos}
y_rep <- as.matrix(fitted_model_NB_hier_mos, pars = "y_rep")
ppc_dens_overlay(
  y = stan_dat_hier$complaints, 
  yrep = y_rep[1:200,]
)
```


```{r}
ppc_stat_grouped(
  y = stan_dat_hier$complaints, 
  yrep = y_rep, 
  group = stan_dat_hier$mo_idx, 
  stat = 'mean'
) + xlim(0, 10)
```

As we can see, it appears that our monthly random intercept has captured a monthly
pattern across all the buildings.

## Using our model: Cost forecasts

We've fitted a nice model, so we can go ahead an use the model to help us make a decision
about how many traps to put in our buildings. We'll make a forecast for 6 months forward.

```{r comp-rev, cache=T, results="hide", message=FALSE}
comp_rev <- stan_model('stan_programs/hier_NB_regression_ncp_slopes_mod_mos_predict.stan')
```

```{r run-NB-hier-rev, cache=TRUE}
stan_dat_hier$log_sq_foot_pred <- log(unique(pest_data$total_sq_foot)/1e4)
stan_dat_hier$M_forward <- 12
rev_model <- sampling(comp_rev, data = stan_dat_hier, chains = 4, cores = 4, iter = 2000)
```

Below we've generated our revenue curves for the 10 buildings. These charts will give us 
precise quantification of our uncertainty around our revenue projections at any number of traps for
each building. 

A key input to our analysis will be the cost of installing bait stations. We're simulating
the number of complaints we receive over the course of a year, so we need to understand the
cost associated with maintaining each bait station over the course of a year. There's the
cost attributed to the raw bait station, which is the plastic housing and the bait material,
a peanut-buttery substance that's injected with insecticide.

```{r rev-curves}
# extract as a list for convenience below
samps_rev <- rstan::extract(rev_model)

N_traps <- 20
costs <- 20*(1:N_traps) + (1:N_traps < 5)*20 + (1:N_traps >= 5 & 1:N_traps < 10)*50 + (1:N_traps >= 10 & 1:N_traps < 15)*80 +  + (1:N_traps >= 15) * 110
tot_profit <- sweep(samps_rev$rev_rep_potential,c(3),STATS = costs, FUN = '-')
mean_profit <- apply(tot_profit,c(2,3),median)
lower_profit <- apply(tot_profit,c(2,3),quantile,0.25)
upper_profit <- apply(tot_profit,c(2,3),quantile,0.75)
profit_df <- data.frame(profit = as.vector(t(mean_profit)), lower = as.vector(t(lower_profit)),
                     upper = as.vector(t(upper_profit)),
                     traps = rep(1:N_traps,N_buildings),
                     building_id = as.vector(sapply(1:N_buildings,rep,N_traps)))
                     # qty = as.vector(t(mean_qty)),
                     # lower_qty = as.vector(t(lower_qty)),
                     # upper_qty = as.vector(t(upper_qty)))
#betas <- order(colMeans(samps_rev$betas))[c(1:5, (N_titles - 4):N_titles)]
#sub_df <- rev_df %>% filter(ttl_idx %in% betas)
ggplot(data = profit_df, aes(x = traps, y = profit)) +
  geom_ribbon(aes(ymin = lower, ymax = upper), fill="grey70") + geom_line() +
#  geom_line() + 
  facet_wrap(~ building_id, scales = 'free_y')
```

Left as an exercise for the reader: 

Let's say our utility function is revenue. If we wanted to maximize expected
revenue, we can take expectations at each station count for each building, and
choose the trap numbers that maximizes expected revenue. This will be called a
maximum revenue strategy.

How can we generate the distribution of portfolio revenue (i.e. the sum of 
revenue across all the buildings) under the maximum revenue strategy from the
posterior draws of $\texttt{rev_rep_potential}$ we already have from
$\texttt{samps_rev}$?
