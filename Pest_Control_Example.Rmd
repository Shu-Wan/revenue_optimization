---
title: "Pest Control"
author: "Lauren Kennedy, Jonah Gabry, & Rob Trangucci"
date: "July 23, 2018"
output:
  pdf_document: default
  html_document: default
---

## Setup

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(rstan)
library(dplyr)
library(reshape2)
library(bayesplot)
library(ggplot2)

theme_set(bayesplot::theme_default())

# for R's pseudo-RNGs, not Stan's
set.seed(1123) 
```

## The problem

LLet's say that you're a data scientist working as an independent contractor. You're contacted by the property manager of a large property network in New York City. They explain that they are having an issue with the number of cockroach complaints that they receive from their buildings. Previously they have offered monthly visits from a pest inspector as a solution to this problem. While this is the default solution of many property managers in NYC, the tennants are rarely home when the inpector visits, and so the manager reasons that this is a relatively expensive solution that is currently not very effective.

One alternative to this problem is to deploy long term bait stations. In this alternative child and pet safe bait stations are installed throughout the apartment building. Cockroaches obtain quick acting poison from these stations distribute it throughout the colony. The manufactorer for these bai stations provides some indication of the space to bait efficacy, but the manager suspects that this guidance was not calculated for NYC roaches. NYC roaches, the manager rationalises, have more hustle than traditional roaches; and NYC buildings are built differently to other common domestic buildings in the US. This is particularly important as the unit cost for each bait station is quite high.

The manager wishes to employ your services to help them to optomize the number of roach bait stations they should place in each of their buildings in order to minimise both the number of cockroach complaints and expenditure on pest control. They have randomly selected `r N_buildings` buildings. At the beginning of each month, a pest inspector randomly places a number of bait stations throughout the building, without knowledge of the current cockroach levels in the building. At the end of the month, the manager records the total number of cockroach complaints in that building. The manager would like to predict the optimise number of traps ($t$) and complaints ($c$) for these `r N_buildings` 10 buildings, and predict for new buildings how many bait stations they should place. 

\begin{align*}
\arg\min_{t \in T} \mathbb{E}_c[c(t) \, t]
\end{align*}

As the property manager has complete control over the number of traps set, the random variable contributing to this expectation is the number of complaints given the number of traps. We will model the number of complaints as a function of the number of traps. 

## The data

The data is in a file called $\texttt{building_data_20180723.RDS}$. Let's load the data and see what the structure
is:

```{r load-data}
pest_data <- readRDS('data/building_data_20180724.RDS')
head(pest_data)
```

```{r describe-data}
N_buildings <- length(unique(pest_data$building_id))
N_buildings
```

We're working with `r N_buildings` buildings. We should first want to learn whether the number of complaints is
related to number of cockroach traps. 

## Bayesian workflow

Insert steps here

## Modeling count data : Poisson distribution

How can we model the number of complaints? We already know some rudimentary information about what we should expect. The number of complaints over a month should be either zero or an integer. The property manager tells us that it is possible but unlikely that number of complaints in a given month is zero. Occasionally there are a very large number of complaints in a single month. A common way of modelling this sort of skewed, single bounded count data is as a Poisson random variable. One concern about modelling the outcome variable as a Poisson is that the data may be over-dispersed. How can we address this concern? We'll start with a simple Poisson model and build it up slowly.

### Model 

Given that we have chosen a Poisson regression, we define the likelihood to be the Poisson probability mass function over the number bait stations placed in the building, denoted below as $\texttt{qty_traps}$. This model assumes that the mean and variance of the outcome variable (in this case the number of complaints) is the same.

\begin{align*}
\texttt{qty_complaints}_{traps} & \sim \text{Poisson}(\lambda_{traps}) \\
\eta_{traps} & = \alpha + \beta \, x_{traps}
\end{align*}
 
### Data

Let's prep the data. Firstly to use the building variable in Stan we will need to trasnform it from a factor variable to an interger variable. 

```{r prep-data}
N_months <- max(table(pest_data$building_id))
pest_data <- pest_data %>%
  mutate(building_fac = as.factor(building_id),
         building_idx = as.integer(building_fac),
         ids = rep(1:N_months, N_buildings),
         mo_idx = lubridate::month(date))
building_data <- 
  pest_data[,c('building_idx','live_in_super', 'age_of_building', 'total_sq_foot',
               'average_tenant_age', 'monthly_average_rent')] %>% unique() %>%
  arrange(building_idx) %>% select(-building_idx) %>% as.matrix() %>% 
  scale(scale=F) %>% sweep(MARGIN = 2, STATS=c(1,10,1e4,10,1e3),FUN = '/')
stan_dat_hier <- 
  with(pest_data, 
        list(complaints = complaints,
             traps = traps,
             N = length(traps),
             J = N_buildings,
             M = N_months,
             sq_foot = log(pest_data$total_sq_foot/1e4),
             building_data = building_data[,-3],
             mo_idx = as.integer(as.factor(date)),
             K = 4,
             building_idx = building_id
             )
        )
```

We will also make a list of the data that we will use for this model as well. 

```{r stan-data}
stan_dat_simple <- list(
  N = nrow(pest_data), 
  complaints = pest_data$complaints,
  traps = pest_data$traps
)
```

### Simulate some data

How do we know if our Stan model is working well and if we are able to recover the known parameter values? Before we start fitting our Poisson model to the real data, first let's generate some fake data that matches our assumptions about the data. 

First we will compile the Stan model (simple_poisson_regression_dgp.stan) that generated the fake data.

```{r compNBdgp, cache=T, results="hide", message=FALSE}
comp_dgp_simple <- stan_model('stan_programs/simple_poisson_regression_dgp.stan')
```

Next we use this model to sample some data. 

```{r runpoissondgp}
fitted_model_dgp <- sampling(
  comp_dgp_simple,
  data = list(N = nrow(pest_data), mean_traps = mean(pest_data$traps)),
  chains = 1,
  iter = 1,
  algorithm = 'Fixed_param',
  seed = 123
  )
samps_dgp <- rstan::extract(fitted_model_dgp)
str(samps_dgp)
```

Like before, we need to arrange the data into a list ready to pass to Stan. 

```{r P_fake_stan_dat}
stan_dat_fake <- list(
  N = nrow(pest_data), 
  traps = samps_dgp$traps[1, ], 
  complaints = samps_dgp$complaints[1, ]
)
str(stan_dat_fake)
```
### Fit the model to the fake data:

Now we have the simulated data we fit a Stan model using it. First we need to compile the model (simple_poisson_regression.stan).

```{r compNB, cache=T, results="hide", message=FALSE}
comp_model_P <- stan_model('stan_programs/simple_poisson_regression.stan')
```

Lastly, let's run the model to see if we can recover our simulated parameters. 

```{r runPoverfake}
fit_model_P <- sampling(comp_model_P, data = stan_dat_fake)

# see http://mc-stan.org/rstan/articles/stanfit_objects.html for various
# ways of extracting the contents of the stanfit object
posterior_alpha_beta <- as.matrix(fit_model_P, pars = c('alpha','beta'))
head(posterior_alpha_beta)
```

### Assess parameter recovery

First explore if we can recover the data that we originally simulated from.

```{r}
true_alpha_beta <- c(samps_dgp$alpha, samps_dgp$beta)
mcmc_recover_hist(posterior_alpha_beta, true = true_alpha_beta)
```

Then do Posterior Predictive Checks

```{r marginal_PPC}
y_rep <- as.matrix(fit_model_P, pars = "y_rep")
mean_y_rep <- colMeans(y_rep)
std_resid <- (stan_dat_fake$complaints - mean_y_rep) / sqrt(mean_y_rep)
qplot(mean_y_rep, std_resid) + hline_at(1) + hline_at(-1)
```
This is a plot of... If the model was fitting well it would look like... As you can see here, we have ... which indicates that ... Finally look at the rootogram. 

```{r}
ppc_rootogram(stan_dat_fake$complaints, yrep = y_rep)
```
This is a plot of... If the model was fitting well it would look like... As you can see here, we have ... which indicates that ...


### Fit with real data 

Now we have seen that we can sensibly recover the parameters from simulated data. Next we can use this model with the real data that we observerd. As we have already compiled the model, we can jump straight to sampling from it. 

```{r fit_P_real_data, cache=T}
fit_P_real_data <- sampling(comp_model_P, data = stan_dat_simple, chains = 4, cores =4 )
```
and printing the parameters. What do these tell us? 

```{r results_simple_P}
print(fit_P_real_data, pars = c('alpha','beta'))
```
As we expected, it appears the number of bait stations set in a building impacts the number of complaints about cockroaches that were made in the follwoing month. However, we still need to consider how well the model fits. 

```{r marginal_PPC}
y_rep <- as.matrix(fit_P_real_data, pars = "y_rep")
bayesplot::ppc_dens_overlay(stan_dat_simple$complaints, y_rep[1:200,])
```
This is a plot of... If the model was fitting well it would look like... As you can see here, we have ... which indicates that ...


```{r marginal_PPC}
mean_y_rep <- colMeans(y_rep)
std_resid <- (stan_dat_simple$complaints - mean_y_rep) / sqrt(mean_y_rep)
qplot(mean_y_rep, std_resid) + hline_at(1) + hline_at(-1)
```
This is a plot of... If the model was fitting well it would look like... As you can see here, we have ... which indicates that ...

```{r}
ppc_rootogram(stan_dat_simple$complaints, yrep = y_rep)
```
This is a plot of... If the model was fitting well it would look like... As you can see here, we have ... which indicates that ...

### Is there more info we can use?

Modeling the relationship between complaints and bait stations is the simplest model. However, the manager has told us that they expect there are a number of other reasons that one building might have more roach complaints than another. 

#### Data

A quick test shows us that there appears to be a relationship between the square footage of the building and the number of complaints received. 

```{r}
sq_foot_dat_lm <- lm(log1p(complaints) ~ I(total_sq_foot/1e4), data = pest_data)
with(pest_data, plot(total_sq_foot/1e4, log1p(complaints)))
abline(reg = sq_foot_dat_lm)
```
Using the property manager's intuition, we include two extra peices of information we know about the building - the square floor space and whether there is a live in super or not - into both the simulated and real data. 
 
```{r}
stan_dat_simple$sq_foot <- log(pest_data$total_sq_foot/1e4)
stan_dat_simple$live_in_super <- pest_data$live_in_super
```

<!-- [Note to Jonah/Rob: Does this need to be here if we generate new fake data below? -->

```{r}
stan_dat_fake$sq_foot <- rnorm(stan_dat_fake$N)
stan_dat_fake$live_in_super <- rnorm(stan_dat_fake$N)
```

####Model

Now we need a new Stan model that uses multiple predictors. Firs we have to compile multiple_poisson_regression_dgp.stan.

```{r compmultPDGP, cache=T, results="hide", message=FALSE}
comp_dgp_multiple <- stan_model('stan_programs/multiple_poisson_regression_dgp.stan')
```

Then we generate some fake data from this model. 

```{r runpoissondgp}
fitted_model_dgp <-
  sampling(
  comp_dgp_multiple,
  data = list(N = nrow(pest_data)),
  chains = 1,
  cores = 1,
  iter = 1,
  algorithm = 'Fixed_param',
  seed = 123
  )
samps_dgp <- rstan::extract(fitted_model_dgp)
```

Now pop that data into a list ready for Stan. 

```{r P_fake_stan_dat}
stan_dat_fake <- list(
  N = nrow(pest_data), 
  sq_foot = samps_dgp$sq_foot[1, ],
  live_in_super = samps_dgp$live_in_super[1, ],
  traps = samps_dgp$traps[1, ], 
  complaints = samps_dgp$complaints[1, ]
)
```

And compile the model to fit this data with. 

```{r compNB, cache=T, results="hide", message=FALSE}
comp_model_P_mult <- stan_model('stan_programs/multiple_poisson_regression.stan')
```

Fit the data with the model, and extract the alpha and beta parameters. 

```{r runPoverfake}
fit_model_P_mult <- sampling(comp_model_P_mult, data = stan_dat_fake, chains = 4, cores = 4)
posterior_alpha_beta <- as.matrix(fit_model_P_mult, pars = c('alpha','beta','beta_super','beta_sq_foot'))
```
Then compare these parameters to the true parameters

```{r}
true_alpha_beta <- c(samps_dgp$alpha,samps_dgp$beta,samps_dgp$beta_super,samps_dgp$beta_sq_foot)
bayesplot::mcmc_recover_hist(posterior_alpha_beta, true = true_alpha_beta)
```
This is a plot of... If the model was fitting well it would look like... As you can see here, we have ... which indicates that ...

### Fit the real data

Now let's fit to real data.

First fit the real data with the model.

```{r fit_mult_P_real_dat}
fit_model_P_mult_real <- sampling(comp_model_P_mult, data = stan_dat_simple, cores = 4, chains = 4)
```
Then explore the fit. 

```{r marginal_PPC}
y_rep <- as.matrix(fit_model_P_mult_real, pars = "y_rep")
bayesplot::ppc_dens_overlay(stan_dat_simple$complaints, y_rep[1:200,])
```
This is a plot of... If the model was fitting well it would look like... As you can see here, we have ... which indicates that ...

```{r marginal_PPC}
mean_y_rep <- colMeans(y_rep)
std_resid <- (stan_dat_simple$complaints - mean_y_rep) / sqrt(mean_y_rep)
qplot(mean_y_rep, std_resid) + hline_at(1) + hline_at(-1)
```
This is a plot of... If the model was fitting well it would look like... As you can see here, we have ... which indicates that ...


```{r}
ppc_rootogram(stan_dat_simple$complaints, yrep = y_rep)
```
This is a plot of... If the model was fitting well it would look like... As you can see here, we have ... which indicates that ...

## Modeling count data : Negative Binomial outcome

We'll start with a simple model and build it up slowly. First, we'll define our 
likelihood to be the negative binomial probability mass function over the number
of units sold, denoted below as $\texttt{qty_sld}$.

\begin{align*}
\texttt{complaints}_{b,t} & \sim \text{Neg-Binomial-log}(\eta_{b,t}, \phi) \\
\eta_{b,t} & = \alpha + \beta \, X_{b,t}
\end{align*}

The negative binomial mass function we'll use is called 
$\texttt{neg_binomial_2_log}(\text{ints} \, y, \text{reals} \, \eta, \text{reals} \, \phi)$ in Stan.
This is negative binomial mass function that is parameterized in terms of its 
log-mean, $\eta$, and its precision $\phi$. This means that $\mathbb{E}[y] \, = exp(\eta)$ 
and $\text{Var}[y] = \exp(\eta) + \exp(\eta)^2 / \phi$. 

We saw that the residuals from the model fitted to the real data didn't
conform to the residuals from the model fitted to the fake data. In particular
the residuals from the real data fit appeared to have variances that were larger
than means. 

We'll go through the same process we went through with the Poisson models above
but instead using a negative binomial outcome model.

###Fake data fit: Multiple NB regression

```{r compNBdgp, cache=T, results="hide", message=FALSE}
comp_dgp_multiple_NB <- stan_model('stan_programs/multiple_NB_regression_dgp.stan')
```

We're going to generate one draw from the fake data model so we can use the data to fit our
model and compare the known values of the parameters to the posterior density of the parameters.

```{r fake_data_dgp_NB}
fitted_model_dgp_NB <-
  sampling(
  comp_dgp_multiple_NB,
  data = list(N = nrow(pest_data)),
  chains = 1,
  cores = 1,
  iter = 1,
  algorithm = 'Fixed_param',
  seed = 123
  )
samps_dgp_NB <- rstan::extract(fitted_model_dgp_NB)
```

Create a dataset to feed into the Stan model.

```{r NB_fake_stan_dat}
stan_dat_fake_NB <- list(
  N = nrow(pest_data), 
  sq_foot = samps_dgp_NB$sq_foot[1, ],
  live_in_super = samps_dgp_NB$live_in_super[1, ],
  traps = samps_dgp_NB$traps[1, ], 
  complaints = samps_dgp_NB$complaints[1, ]
)
```

Compile the inferential model.

```{r compNB, cache=T, results="hide", message=FALSE}
comp_model_NB <- stan_model('stan_programs/multiple_NB_regression.stan')
```

Now we run our NB regression over the fake data and extract the samples to 
examine posterior predictive checks and to check whether we've sufficiently
recovered our known parameters, $\texttt{alpha}$ $\texttt{beta}$.

```{r runNBoverfake}
fitted_model_NB <- 
  sampling(comp_model_NB,
           data = stan_dat_fake_NB,
           chains = 4, cores = 4
  )
posterior_alpha_beta_NB <- 
  as.matrix(fitted_model_NB,
            pars = c('alpha','beta',
                     'beta_super',
                     'beta_sq_foot',
                     'inv_prec')
  )
```

Construct the vector of true values from your simulated dataset
and compare to the recovered parameters.

```{r}
true_alpha_beta_NB <- 
  c(samps_dgp_NB$alpha,
    samps_dgp_NB$beta,
    samps_dgp_NB$beta_super,
    samps_dgp_NB$beta_sq_foot,
    samps_dgp_NB$inv_prec
  )
bayesplot::mcmc_recover_hist(posterior_alpha_beta_NB, true = true_alpha_beta_NB)
```

[Note to Lauren and Jonah: This looks OK, but what's going on with the 
intercept? I think we need to shift the intercept to deal with the effect from
noncentered parameters, but I'm not sure. It looks like the posterior standard
deviation is larger than the prior standard deviation. Not impossible, but 
still odd for the overall intercept. Also, the 
inverse precision parameter has almost no contraction, which is odd. We should
figure this out before the class]

```{r marginal_PPC}
y_rep <- rstan::extract(fitted_model_NB)$y_rep
mean_inv_prec <- mean(posterior_alpha_beta_NB[,'inv_prec'])
mean_y_rep <- colMeans(y_rep)
std_resid <- (stan_dat_simple$complaints - mean_y_rep) / sqrt(mean_y_rep + mean_y_rep^2*mean_inv_prec)
qplot(mean_y_rep, std_resid) + hline_at(1) + hline_at(-1)
```

Fit the model to the real data.

```{r runNB}
fitted_model_NB <- sampling(comp_model_NB, data = stan_dat_simple, chains = 4, cores = 4)
samps_NB <- rstan::extract(fitted_model_NB)
```

Let's look at our predictions vs. the data.

```{r ppc-full}
y_rep <- samps_NB$y_rep
bayesplot::ppc_dens_overlay(stan_dat_simple$complaints, 
                            y_rep[1:200,])
```

These look OK, but let's look at the standardized residual plot.

```{r marginal_PPC}
mean_inv_prec <- mean(samps_NB$inv_prec)
mean_y_rep <- colMeans(y_rep)
std_resid <- (stan_dat_simple$complaints - mean_y_rep) / sqrt(mean_y_rep + mean_y_rep^2*mean_inv_prec)
qplot(mean_y_rep, std_resid) + hline_at(1) + hline_at(-1)
```

Looks ok, but we still have some large residuals. It might help to look at the data grouped by
building.

```{r}
ppc_rootogram(stan_dat_simple$complaints, yrep = y_rep)
```

The rootgram looks much better, but we should try look at the data along other
axes to see if there's more information we can use to build our model.

```{r ppc-group_means}
bayesplot::ppc_stat_grouped(
  stan_dat_simple$complaints, 
  y_rep[1:200,], 
  group = pest_data$building_idx, 
  stat = 'mean'
)
```


### Hierarchical modeling
#### Random intercepts for each building

It appears that we aren't modeling the means well for each building. Let's add a hierarchical
intercept parameter, $\alpha_b$ at the building level to our model. 

$$
\texttt{complaints}_{b,t} \sim \text{Neg-Binomial-log}(\eta_{b,t}, \phi) \\
\eta_{m,t} = \alpha_b + \beta \, X_{b,t} \\
\alpha_b \sim \text{Normal}(\alpha, \sigma_{\alpha})
$$

In our stan model, $\alpha_b$ is the $b$-th element of the vector
$\texttt{alpha}$ defined in the parameter vector. 

We could also shove the building-only predictors into the model for 
$\alpha_b$, like so:

$$
\alpha_b \sim \text{Normal}(\alpha + \zeta \, \texttt{building_data}_b, \sigma_{\alpha})
$$

Let's compile the model.

```{r comp-NB-hier, cache=T, results="hide", message=FALSE}
comp_model_NB_hier <- stan_model('stan_programs/hier_NB_regression.stan')
```

Fit the model to data.

```{r run-NB-hier}
fitted_model_NB_hier <- sampling(comp_model_NB_hier, data = stan_dat_hier, chains = 4, cores = 4)
```

We get a bunch of warnings from Stan about divergent transitions. Divergent transitions are...

We get clues about why Stan is complaining from examining the fit of the model.

```{r} 
samps_hier_NB <- rstan::extract(fitted_model_NB_hier)
```


```{r print-NB-hier}
print(fitted_model_NB_hier, pars = c('sigma_alpha','beta','alpha','prec','alphas'))
```

The effective samples are quite low for many of the parameters. This indicates
that there are problems with our parameterization of the model. We can reparameterize
the random intercept $\alpha_b$, which is distributed:

\begin{align*}
\alpha_b & \sim \text{Normal}(\alpha + \zeta \, \texttt{building_data}, \sigma_{\alpha})
\end{align*}

```{r}
# use as.array to keep the markov chains separate for trace plots
mcmc_trace(
  as.array(fitted_model_NB_hier,pars = 'sigma_alpha'), 
  np = nuts_params(fitted_model_NB_hier)
)
```

```{r}
# assign to object so we can compare to another plot later
scatter_with_divs <- mcmc_scatter(
  as.array(fitted_model_NB_hier),
  pars = c("alphas[4]", 'sigma_alpha'), 
  transform = list('sigma_alpha' = "log"), 
  np = nuts_params(fitted_model_NB_hier)
)
scatter_with_divs
```

What we have here is a funnel, which Stan can't handle because...

```{r}
parcoord_with_divs <- mcmc_parcoord(
  as.array(fitted_model_NB_hier, pars = c("sigma_alpha", "alphas")),
  np = nuts_params(fitted_model_NB_hier)
)
parcoord_with_divs
```

Again, we see evidence that our problems concentrate when $\texttt{sigma_alpha}$ is small

Instead, we should use the non-centered parameterization for $\alpha_b$. We
define a vector of auxiliary variables in the parameters block,
$\texttt{alphas_raw}$ that is given a $\text{Normal}(0, 1)$ prior in the model
block. We then make $\texttt{alphas}$ a transformed parameter:

\begin{verbatim}
transformed parameters {
  vector[J] alphas;
  alphas = alpha + building_data * zeta + sigma_alpha * alphas_raw;
}
\end{verbatim}

This gives $\texttt{alphas}$ a $\text{Normal}(\alpha + \texttt{building_data}\, \zeta, \sigma_\alpha)$ distribution, but it
decouples the dependence of the density of each element of $\texttt{alphas}$ from 
$\texttt{sigma_alpha}$ ($\sigma_\alpha$). hier_NB_regression_ncp.stan uses the non-centered
parameterization for $\texttt{alphas}$. We will examine the effective sample size of the
fitted model to see whether we've fixed the problem with our reparameterization.

Compile the model.

```{r comp-NB-hier-ncp}
comp_model_NB_hier_ncp <- stan_model('stan_programs/hier_NB_regression_ncp.stan')
```

Fit the model to the data.

```{r run-NB-hier-ncp}
fitted_model_NB_hier_ncp <- sampling(comp_model_NB_hier_ncp, data = stan_dat_hier, chains = 4, cores = 4)
```

Examining the fit of the new model

```{r n-eff-NB-hier-ncp-check}
print(fitted_model_NB_hier_ncp, pars = c('sigma_alpha','beta','alpha','prec','alphas'))
```

This has improved the effective sample sizes of $\texttt{alphas}$. We extract
the parameters to run our usual posterior predictive checks.

```{r}
scatter_no_divs <- mcmc_scatter(
  as.array(fitted_model_NB_hier_ncp),
  pars = c("alphas[4]", 'sigma_alpha'), 
  transform = list('sigma_alpha' = "log"), 
  np = nuts_params(fitted_model_NB_hier_ncp)
)
bayesplot_grid(scatter_with_divs, scatter_no_divs, 
               grid_args = list(ncol = 2), ylim = c(-11, -0.5))
```

```{r}
parcoord_no_divs <- mcmc_parcoord(
  as.array(fitted_model_NB_hier_ncp, pars = c("sigma_alpha", "alphas")),
  np = nuts_params(fitted_model_NB_hier_ncp)
)
bayesplot_grid(parcoord_with_divs, parcoord_no_divs, 
               ylim = c(-3, 3))
```

```{r samps-full-hier}
samps_NB_hier_ncp <- rstan::extract(fitted_model_NB_hier_ncp, pars = c('y_rep','inv_prec'))
```

The marginal plot, again.

```{r ppc-full-hier}
y_rep <- as.matrix(fitted_model_NB_hier_ncp, pars = "y_rep")
ppc_dens_overlay(stan_dat_hier$complaints, y_rep[1:200,])
```

This looks quite nice. If we've captured the building-level means well, then the
posterior distribution of means by building should match well with the observed
means of the quantity of building complaints by month.

```{r ppc-group_means-hier}
ppc_stat_grouped(
  y = stan_dat_hier$complaints, 
  yrep = y_rep,
  group = stan_dat_hier$building_idx, 
  stat = 'mean'
)
```

The building means appear to be well-captured by our model.


```{r}
mean_y_rep <- colMeans(y_rep)
mean_inv_prec <- mean(as.matrix(fitted_model_NB_hier_ncp, pars = "inv_prec"))
std_resid <- (stan_dat_hier$complaints - mean_y_rep) / sqrt(mean_y_rep + mean_y_rep^2*mean_inv_prec)
qplot(mean_y_rep, std_resid) + hline_at(1) + hline_at(-1)
```

```{r}
ppc_rootogram(stan_dat_hier$complaints, yrep = y_rep)
```


Ok, so these plots look \emph{much} better. Perhaps if the levels of complaints
differ by building, the coefficient for the effect of traps on building does
too. We can add this to our model and observe the fit.

$$
\text{complaints}_{b,t} \sim \text{Neg-Binomial-log}(\eta_{b,t}, \phi)  \\
\eta_{b,t} = \alpha_b + \beta_b \, \texttt{traps}_{b,t} \\
\alpha_b \sim \text{Normal}(\alpha + \texttt{building_data} \, \zeta, \sigma_{\alpha}) \\
\beta_b \sim \text{Normal}(\beta + \texttt{building_data} \, \gamma, \sigma_{\beta})
$$

Let's compile the model.

```{r comp-NB-hier-slopes, cache=T, results="hide", message=FALSE}
comp_model_NB_hier_slopes <- stan_model('stan_programs/hier_NB_regression_ncp_slopes_mod.stan')
```

Fit the model to data and extract the posterior draws needed for 
our posterior predictive checks.

```{r run-NB-hier-slopes}
fitted_model_NB_hier_slopes <- 
  sampling(
    comp_model_NB_hier_slopes,
    data = stan_dat_hier,
    chains = 4, cores = 4
  )
```

To see if the model infers building-to-building differences in, we can plot a histogram of our marginal
posterior distribution for $\texttt{sigma_beta}$. 

```{r}
mcmc_hist(
  as.matrix(fitted_model_NB_hier_slopes, pars = "sigma_beta"), 
  binwidth = 0.005
)
```

While the model can't specifically rule out zero from the posterior, it does have mass at small
non-zero numbers, so we should leave in the hierarchy over $\texttt{betas}$. Plotting the marginal
data density again, we can see the model still looks well calibrated.

```{r ppc-full-hier-slopes}
y_rep <- as.matrix(fitted_model_NB_hier_slopes, pars = "y_rep")
ppc_dens_overlay(
  y = stan_dat_hier$complaints, 
  yrep = y_rep[1:200,]
)
```

[It might be interesting to do more investingation of the model, but I haven't added this yet]
[We haven't done anything about inference really, we're just hammering in the posterior predictive
checks. This could motivate informative priors for some of the zeta/gamma regression parameters]

### Time varying effects and structured priors

We haven't looked at how cockroach complaints change over time. Let's look
at whether there's any pattern over time.

```{r ppc-group_max-hier-slopes-mean-by-mo}
ppc_stat_grouped(
  y = stan_dat_hier$complaints, 
  yrep = y_rep, 
  group = stan_dat_hier$mo_idx, 
  stat = 'mean'
) + xlim(0, 9)
```

There's definitely some pattern that we're not capturing. See October and May and June for examples.
If we think about the sort of prior we want to use for this parameter, we might settle on
an autoregressive prior.

[This doesn't quite come out from any residuals because we have so few weeks of data]

We might be missing information in month indicators. It makes sense, when it gets colder
out there might be less trash in the streets, which could mean fewer cockroaches are out
and about rooting through the trash and ending up in our buildings.

This can be a motivation for using an autoregressive prior for our monthly effects. The model
is as follows:

$$
\texttt{mo}_t \sim \text{Normal}(\phi \, \texttt{mo}_{t-1}, \sigma_\texttt{mo}) \\
\phi \in [0,1]
$$
While we could allow $\phi \in [-1,1]$, we almost never see $\phi < 0$ in social science
applications. We could relax this assumption and use priors to enforce our beliefs

[Note to L/J: we should probably just use priors to encode our
beliefs about positive autocorrelation this upon second thought]

```{r comp-NB-hier-mos, cache=T, results="hide", message=FALSE}
comp_model_NB_hier_mos <- stan_model('stan_programs/hier_NB_regression_ncp_slopes_mod_mos.stan')
```

```{r run-NB-hier-slopes-mos}
fitted_model_NB_hier_mos <- sampling(comp_model_NB_hier_mos, data = stan_dat_hier, chains = 4, cores = 4)
```


In the interest of brevity, we won't go on expanding the model. 

```{r ppc-full-hier-mos}
y_rep <- as.matrix(fitted_model_NB_hier_mos, pars = "y_rep")
bayesplot::ppc_dens_overlay(
  y = stan_dat_hier$complaints, 
  yrep = y_rep[1:200,]
)
```


```{r}
ppc_stat_grouped(
  y = stan_dat_hier$complaints, 
  yrep = y_rep, 
  group = stan_dat_hier$mo_idx, 
  stat = 'mean'
) + xlim(0, 10)
```

As we can see, it appears that our monthly random intercept has captured a monthly
pattern across all the buildings.

## Using our model: Cost forecasts

We've fitted a nice model, so we can go ahead an use the model to help us make a decision
about how many traps to put in our buildings. We'll make a forecast for 6 months forward.

```{r comp-rev, cache=T, results="hide", message=FALSE}
comp_rev <- stan_model('stan_programs/hier_NB_regression_ncp_slopes_mod_mos_predict.stan')
```

```{r run-NB-hier-rev, cache=TRUE}
stan_dat_hier$sq_foot_pred <- log(unique(pest_data$total_sq_foot)/1e4)
stan_dat_hier$M_forward <- 12
rev_model <- sampling(comp_rev, data = stan_dat_hier, chains = 4, cores = 4, iter = 2000)
```

Below we've generated our revenue curves for the 10 buildings. These charts will give us 
precise quantification of our uncertainty around our revenue projections at any number of traps for
each building. 

```{r rev-curves}
# extract as a list for convenience below
samps_rev <- rstan::extract(rev_model)

N_traps <- 20
costs <- 20*(1:N_traps) + (1:N_traps < 5)*20 + (1:N_traps >= 5 & 1:N_traps < 10)*50 + (1:N_traps >= 10 & 1:N_traps < 15)*80 +  + (1:N_traps >= 15) * 110
tot_profit <- sweep(samps_rev$rev_rep_potential,c(3),STATS = costs, FUN = '-')
mean_profit <- apply(tot_profit,c(2,3),median)
lower_profit <- apply(tot_profit,c(2,3),quantile,0.25)
upper_profit <- apply(tot_profit,c(2,3),quantile,0.75)
profit_df <- data.frame(profit = as.vector(t(mean_profit)), lower = as.vector(t(lower_profit)),
                     upper = as.vector(t(upper_profit)),
                     traps = rep(1:N_traps,N_buildings),
                     building_id = as.vector(sapply(1:N_buildings,rep,N_traps)))
                     # qty = as.vector(t(mean_qty)),
                     # lower_qty = as.vector(t(lower_qty)),
                     # upper_qty = as.vector(t(upper_qty)))
#betas <- order(colMeans(samps_rev$betas))[c(1:5, (N_titles - 4):N_titles)]
#sub_df <- rev_df %>% filter(ttl_idx %in% betas)
ggplot(data = profit_df, aes(x = traps, y = profit)) +
  geom_ribbon(aes(ymin = lower, ymax = upper), fill="grey70") + geom_line() +
#  geom_line() + 
  facet_wrap(~ building_id, scales = 'free_y')
```

Left as an exercise for the reader: 

Let's say our utility function is revenue. If we wanted to maximize expected
revenue, we can take expectations at each station count for each building, and
choose the trap numbers that maximizes expected revenue. This will be called a
maximum revenue strategy.

How can we generate the distribution of portfolio revenue (i.e. the sum of 
revenue across all the buildings) under the maximum revenue strategy from the
posterior draws of $\texttt{rev_rep_potential}$ we already have from
$\texttt{samps_rev}$?